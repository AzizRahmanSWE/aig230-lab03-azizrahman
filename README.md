# AIG 230 â€“ Lab 03: Text Representation & Statistical Language Models

This repository contains my implementation and analysis for Lab 03, focusing on the transition from raw text to structured statistical models.

## Project Summary

In this lab, I explored the fundamental techniques used to represent text numerically and the construction of probabilistic language models. The work is divided into feature extraction, similarity analysis, and N-gram modeling.

### Key Components

- **Feature Engineering:** Implemented **Bag-of-Words** and **TF-IDF** to transform text into vector space.
- **Text Analytics:** Conducted similarity assessments and classification tasks to evaluate representation quality.
- **Language Modeling:** Built and trained **Unigram, Bigram, and Trigram** models from scratch.
- **Model Evaluation:** Utilized **Perplexity** metrics to quantify the predictive performance of the statistical models.

## Completed Work

- [x] Feature extraction notebooks
- [x] N-gram model implementation
- [x] Perplexity-based evaluation
- [x] Final analysis and results

## Repository Structure

- `notebook1.ipynb`: Focuses on BoW, TF-IDF, and text similarity.
- `notebook2.ipynb`: Focuses on N-gram construction and evaluation.

---
*Completed by Aziz Rahman as part of the AIG 230 curriculum.*
